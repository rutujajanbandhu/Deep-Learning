{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#DEEP LEARNING\n",
        "##LAB 8"
      ],
      "metadata": {
        "id": "C_Wcv_BA2stB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAV1XLn73Y_E",
        "outputId": "e8e346f2-1a11-40fb-a914-33151b47c870"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "import itertools\n",
        "from torchvision.datasets import ImageFolder\n",
        "from dataset import HorseZebraDataset\n",
        "from discriminator_model import Discriminator\n",
        "from generator_model import Generator\n",
        "from utils import save_checkpoint, load_checkpoint\n",
        "from tqdm import tqdm\n",
        "from torchvision.utils import save_image\n",
        "import config\n"
      ],
      "metadata": {
        "id": "ZHM4GCmaBwWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root_A, root_B, transforms_=None):\n",
        "        self.transform = transforms.Compose(transforms_)\n",
        "        self.files_A = sorted(glob.glob(os.path.join(root_A, \"*.*\")))\n",
        "        self.files_B = sorted(glob.glob(os.path.join(root_B, \"*.*\")))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        item_A = self.transform(Image.open(self.files_A[index % len(self.files_A)]))\n",
        "        item_B = self.transform(Image.open(self.files_B[index % len(self.files_B)]))\n",
        "        return {\"A\": item_A, \"B\": item_B}\n",
        "\n",
        "    def __len__(self):\n",
        "        return max(len(self.files_A), len(self.files_B))\n",
        "\n",
        "batch_size = 1\n",
        "data_transforms = [\n",
        "    transforms.Resize(int(286 * 1.12), Image.BICUBIC),\n",
        "    transforms.RandomCrop((256, 256)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "]\n",
        "\n",
        "dataset = ImageDataset(root_A='/content/drive/MyDrive/Deep Learning/archive/trainA', root_B='/content/drive/MyDrive/Deep Learning/archive/trainB', transforms_=data_transforms)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "metadata_df = pd.read_csv('/content/drive/MyDrive/Deep Learning/mnist_train.csv')\n"
      ],
      "metadata": {
        "id": "pEU7H310JwL5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab70b4ef-90d3-46d1-98ae-6f1b4117b1d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv5 = nn.Conv2d(512, 512, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = F.relu(self.conv5(x))\n",
        "        return x\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv5 = nn.Conv2d(512, 1, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.conv1(x), negative_slope=0.2)\n",
        "        x = F.leaky_relu(self.conv2(x), negative_slope=0.2)\n",
        "        x = F.leaky_relu(self.conv3(x), negative_slope=0.2)\n",
        "        x = F.leaky_relu(self.conv4(x), negative_slope=0.2)\n",
        "        x = torch.sigmoid(self.conv5(x))\n",
        "        return x\n",
        "\n",
        "G_AB = Generator()\n",
        "G_BA = Generator()\n",
        "D_A = Discriminator()\n",
        "D_B = Discriminator()\n",
        "\n",
        "print(G_AB)\n",
        "print(D_A)\n"
      ],
      "metadata": {
        "id": "jl-2tdVDJwJa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f86d699f-1f03-4121-f994-ceb4335c5dca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "  (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "  (conv4): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "  (conv5): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            ")\n",
            "Discriminator(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "  (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "  (conv4): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "  (conv5): Conv2d(512, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SpatialTransformer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(SpatialTransformer, self).__init__()\n",
        "        self.localization = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.fc_loc = nn.Sequential(\n",
        "            nn.Linear(128 * 7 * 7, 256),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(256, 2 * 3)\n",
        "        )\n",
        "        self.fc_loc[2].weight.data.zero_()\n",
        "        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
        "\n",
        "    def forward(self, x):\n",
        "        xs = self.localization(x)\n",
        "        xs = xs.view(-1, 128 * 7 * 7)\n",
        "        theta = self.fc_loc(xs).view(-1, 2, 3)\n",
        "        grid = F.affine_grid(theta, x.size())\n",
        "        x = F.grid_sample(x, grid)\n",
        "        return x\n",
        "\n",
        "class GeneratorSTN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GeneratorSTN, self).__init__()\n",
        "        self.stn = SpatialTransformer(in_channels=3, out_channels=3)\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.norm1 = nn.InstanceNorm2d(64)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.norm2 = nn.InstanceNorm2d(128)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.norm3 = nn.InstanceNorm2d(256)\n",
        "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
        "        self.norm4 = nn.InstanceNorm2d(512)\n",
        "        self.conv5 = nn.Conv2d(512, 3, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stn(x)\n",
        "\n",
        "        x = F.relu(self.norm1(self.conv1(x)))\n",
        "        x = F.relu(self.norm2(self.conv2(x)))\n",
        "        x = F.relu(self.norm3(self.conv3(x)))\n",
        "        x = F.relu(self.norm4(self.conv4(x)))\n",
        "        x = torch.tanh(self.conv5(x))\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "m-OMgzoDr_5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GeneratorSemiSupervised(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GeneratorSemiSupervised, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.norm1 = nn.InstanceNorm2d(64)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
        "        self.norm2 = nn.InstanceNorm2d(128)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
        "        self.norm3 = nn.InstanceNorm2d(256)\n",
        "        self.residual_blocks = self.make_res_blocks(256, 6)\n",
        "        self.deconv1 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "        self.norm4 = nn.InstanceNorm2d(128)\n",
        "        self.deconv2 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "        self.norm5 = nn.InstanceNorm2d(64)\n",
        "        self.conv4 = nn.Conv2d(64, 3, kernel_size=3, stride=1, padding=1)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def make_res_blocks(self, channels, num_blocks):\n",
        "        res_blocks = []\n",
        "        for _ in range(num_blocks):\n",
        "            res_blocks.append(ResidualBlock(channels))\n",
        "        return nn.Sequential(*res_blocks)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.norm1(self.conv1(x)))\n",
        "        x = F.relu(self.norm2(self.conv2(x)))\n",
        "        x = F.relu(self.norm3(self.conv3(x)))\n",
        "        x = self.residual_blocks(x)\n",
        "        x = F.relu(self.norm4(self.deconv1(x)))\n",
        "        x = F.relu(self.norm5(self.deconv2(x)))\n",
        "        x = self.tanh(self.conv4(x))\n",
        "        return x\n",
        "\n",
        "class DiscriminatorSemiSupervised(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DiscriminatorSemiSupervised, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
        "        self.norm1 = nn.InstanceNorm2d(128)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)\n",
        "        self.norm2 = nn.InstanceNorm2d(256)\n",
        "        self.conv4 = nn.Conv2d(256, 512, kernel_size=4, stride=1, padding=1)\n",
        "        self.norm3 = nn.InstanceNorm2d(512)\n",
        "        self.conv5 = nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.conv1(x), 0.2)\n",
        "        x = F.leaky_relu(self.norm1(self.conv2(x)), 0.2)\n",
        "        x = F.leaky_relu(self.norm2(self.conv3(x)), 0.2)\n",
        "        x = F.leaky_relu(self.norm3(self.conv4(x)), 0.2)\n",
        "        x = torch.sigmoid(self.conv5(x))\n",
        "        return x\n",
        "\n",
        "class AuxiliaryClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AuxiliaryClassifier, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
        "        self.norm1 = nn.InstanceNorm2d(512)\n",
        "        self.conv2 = nn.Conv2d(512, 1024, kernel_size=3, stride=2, padding=1)\n",
        "        self.norm2 = nn.InstanceNorm2d(1024)\n",
        "        self.conv3 = nn.Conv2d(1024, num_classes, kernel_size=1, stride=1)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.norm1(self.conv1(x)))\n",
        "        x = F.relu(self.norm2(self.conv2(x)))\n",
        "        x = self.conv3(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "XkPNR2WIr_27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adversarial_training(gen_Z, gen_H, disc_Z, disc_H, opt_gen, opt_disc, loader, mse_loss, g_scaler, d_scaler):\n",
        "    loop = tqdm(loader, leave=True)\n",
        "    for idx, (zebra, horse) in enumerate(loop):\n",
        "        zebra = zebra.to(config.DEVICE)\n",
        "        horse = horse.to(config.DEVICE)\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            fake_zebra = gen_Z(horse)\n",
        "            disc_Z_real = disc_Z(zebra)\n",
        "            disc_Z_fake = disc_Z(fake_zebra.detach())\n",
        "            loss_D_Z_real = mse_loss(disc_Z_real, torch.ones_like(disc_Z_real))\n",
        "            loss_D_Z_fake = mse_loss(disc_Z_fake, torch.zeros_like(disc_Z_fake))\n",
        "            loss_D_Z = (loss_D_Z_real + loss_D_Z_fake) / 2\n",
        "\n",
        "        opt_disc.zero_grad()\n",
        "        d_scaler.scale(loss_D_Z).backward()\n",
        "        d_scaler.step(opt_disc)\n",
        "        d_scaler.update()\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            fake_horse = gen_H(zebra)\n",
        "            disc_H_real = disc_H(horse)\n",
        "            disc_H_fake = disc_H(fake_horse.detach())\n",
        "            loss_D_H_real = mse_loss(disc_H_real, torch.ones_like(disc_H_real))\n",
        "            loss_D_H_fake = mse_loss(disc_H_fake, torch.zeros_like(disc_H_fake))\n",
        "            loss_D_H = (loss_D_H_real + loss_D_H_fake) / 2\n",
        "\n",
        "        opt_disc.zero_grad()\n",
        "        d_scaler.scale(loss_D_H).backward()\n",
        "        d_scaler.step(opt_disc)\n",
        "        d_scaler.update()\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            fake_zebra = gen_Z(horse)\n",
        "            fake_horse = gen_H(zebra)\n",
        "            disc_Z_fake = disc_Z(fake_zebra)\n",
        "            disc_H_fake = disc_H(fake_horse)\n",
        "            loss_G_Z = mse_loss(disc_Z_fake, torch.ones_like(disc_Z_fake))\n",
        "            loss_G_H = mse_loss(disc_H_fake, torch.ones_like(disc_H_fake))\n",
        "            cycle_zebra = gen_Z(fake_horse)\n",
        "            cycle_horse = gen_H(fake_zebra)\n",
        "            cycle_loss = config.LAMBDA_CYCLE * (mse_loss(cycle_zebra, zebra) + mse_loss(cycle_horse, horse))\n",
        "            total_loss = loss_G_Z + loss_G_H + cycle_loss\n",
        "\n",
        "        opt_gen.zero_grad()\n",
        "        g_scaler.scale(total_loss).backward()\n",
        "        g_scaler.step(opt_gen)\n",
        "        g_scaler.update()\n",
        "\n",
        "        loop.set_postfix(loss_D_Z=loss_D_Z.item(), loss_D_H=loss_D_H.item(), loss_G=total_loss.item())\n",
        "\n",
        "def main():\n",
        "    gen_Z = Generator(img_channels=3, num_residuals=9).to(config.DEVICE)\n",
        "    gen_H = Generator(img_channels=3, num_residuals=9).to(config.DEVICE)\n",
        "    disc_Z = Discriminator(in_channels=3).to(config.DEVICE)\n",
        "    disc_H = Discriminator(in_channels=3).to(config.DEVICE)\n",
        "    opt_gen = optim.Adam(list(gen_Z.parameters()) + list(gen_H.parameters()), lr=config.LEARNING_RATE, betas=(0.5, 0.999))\n",
        "    opt_disc = optim.Adam(list(disc_Z.parameters()) + list(disc_H.parameters()), lr=config.LEARNING_RATE, betas=(0.5, 0.999))\n",
        "    mse_loss = nn.MSELoss()\n",
        "\n",
        "    dataset = HorseZebraDataset(root_horse=config.TRAIN_DIR + \"/horses\", root_zebra=config.TRAIN_DIR + \"/zebras\", transform=config.transforms)\n",
        "    loader = DataLoader(dataset, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=config.NUM_WORKERS, pin_memory=True)\n",
        "\n",
        "    g_scaler = torch.cuda.amp.GradScaler()\n",
        "    d_scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    for epoch in range(config.NUM_EPOCHS):\n",
        "        adversarial_training(gen_Z, gen_H, disc_Z, disc_H, opt_gen, opt_disc, loader, mse_loss, g_scaler, d_scaler)\n",
        "\n",
        "        if config.SAVE_MODEL:\n",
        "            save_checkpoint(gen_Z, opt_gen, filename=config.CHECKPOINT_GEN_Z)\n",
        "            save_checkpoint(gen_H, opt_gen, filename=config.CHECKPOINT_GEN_H)\n",
        "            save_checkpoint(disc_Z, opt_disc, filename=config.CHECKPOINT_DISC_Z)\n",
        "            save_checkpoint(disc_H, opt_disc, filename=config.CHECKPOINT_DISC_H)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9eIlRtzJt2C",
        "outputId": "4fcc5547-ec40-4ae0-c3e6-d30303d49b86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'dataset'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-78ab1a1d1a7b>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHorseZebraDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdiscriminator_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDiscriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgenerator_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dataset'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dRr5uUxUJtxO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}